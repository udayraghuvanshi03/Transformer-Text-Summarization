{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "\n",
    "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. This project covers the folllowing:\n",
    "\n",
    "- Use built-in functions to preprocess your data\n",
    "- Implement DotProductAttention\n",
    "- Implement Causal Attention\n",
    "- Understand how attention works\n",
    "- Build the transformer model\n",
    "- Evaluate your model\n",
    "- Summarize an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:58:59.642952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742151539.661815   70157 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742151539.667699   70157 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-16 11:58:59.694924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has to be celebrated.\n",
      "Demi: I agree! :D\n",
      "Demi: Tonight at Death & Co.?\n",
      "Lucas: Sure!\n",
      "Lucas: See you there at 10pm?\n",
      "Demi: Yeah! See you there! :D\n",
      "\n",
      "Summary:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/corpus\"\n",
    "\n",
    "train_data, test_data = utils.get_train_test_data(data_dir)\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "First you will do some preprocessing of the data and split it into inputs and outputs. Here you also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end. You will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document, summary = utils.preprocess(train_data)\n",
    "document_test, summary_test = utils.preprocess(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the standard preprocessing with the tensorflow library. You will need to modify the filters, because you dont want the `[EOS]` tokens to be removed.\n",
    "\n",
    "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 34250\n"
     ]
    }
   ],
   "source": [
    "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
    "oov_token = '[UNK]'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
    "\n",
    "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
    "\n",
    "tokenizer.fit_on_texts(documents_and_summary)\n",
    "\n",
    "inputs = tokenizer.texts_to_sequences(document)\n",
    "targets = tokenizer.texts_to_sequences(summary)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'Size of vocabulary: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can pad the tokenized sequences for the training data.\n",
    "\n",
    "For the purpose of this project you need to limit the length of the sequences, as transformers are really big models and are not meant to be trained in such small environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742151544.987562   70157 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "# Pad the sequences.\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data is automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        d_model (int): Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "    \n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "    \n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    # print(pos_encoding.shape)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
    "    \n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits. \n",
    "    # this will allow for broadcasting later when comparing sequences\n",
    "    return seq[:, tf.newaxis, :] \n",
    "\n",
    "\n",
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
    "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
    "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
    "        mask (tf.Tensor): mask with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.linalg.matmul(q,k,transpose_b=True)\n",
    "#     print(k.shape)\n",
    "#     print(q.shape)\n",
    "#     print(v.shape)\n",
    "    # scale matmul_qk with the square root of dk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk/tf.math.sqrt(dk)\n",
    "    # print(mask)\n",
    "    # print(scaled_attention_logits)\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None: \n",
    "        scaled_attention_logits += (1.0-mask)*-1e9\n",
    "    # print(scaled_attention_logits)\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.keras.activations.softmax(\n",
    "    scaled_attention_logits, axis=-1\n",
    ")\n",
    "\n",
    "    # Multiply the attention weights by v\n",
    "    output = tf.linalg.matmul(attention_weights,v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " [[[1.   0.62]\n",
      "  [0.62 0.62]\n",
      "  [0.74 0.31]]]\n",
      "\n",
      "Attention weights:\n",
      " [[[0.   0.38 0.   0.23 0.38]\n",
      "  [0.38 0.   0.   0.23 0.38]\n",
      "  [0.26 0.43 0.   0.16 0.16]]]\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "q = np.array([[1, 1, 0, 1], [0, 1, 1, 1], [1, 0, 1, 1]]).astype(np.float32)\n",
    "k = np.array([[1, 1, 0, 1], [1, 0, 1, 1 ], [1, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]).astype(np.float32)\n",
    "v = np.array([[0, 0], [1, 0], [1, 0], [1, 1], [1, 1]]).astype(np.float32)\n",
    "mask = np.array([[[0, 1, 0, 1, 1], [1, 0, 0, 1, 1], [1, 1, 0, 1, 1]]])\n",
    "\n",
    "ou, atw = scaled_dot_product_attention(q, k, v, mask)\n",
    "ou = np.around(ou, decimals=2)\n",
    "atw = np.around(atw, decimals=2)\n",
    "\n",
    "print(f\"Output:\\n {ou}\")\n",
    "print(f\"\\nAttention weights:\\n {atw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder.\n",
    "\n",
    "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    \"\"\"\n",
    "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
    "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
    "    embedding_dim and no activation.\n",
    "\n",
    "    Arguments:\n",
    "        embedding_dim (int): output dimension\n",
    "        fully_connected_dim (int): dimension of the hidden layer\n",
    "\n",
    "    Returns:\n",
    "        _ (tf.keras.Model): sequential model\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training.\n",
    "\n",
    " The encoder block performs the following steps: \n",
    "1. It takes the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K are the same. You will also perform Dropout in this multi-head attention layer during training. \n",
    "2. There is a skip connection to add your original input `x` and the output of the multi-head attention layer. \n",
    "3. After adding the skip connection, the output passes through the first normalization layer.\n",
    "4. Finally, steps 1-3 are repeated but with the feed forward neural network with a dropout layer instead of the multi-head attention layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This architecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha(~1 line).\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # skip connection\n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer\n",
    "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output during training\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
    "        # to get the output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Encoder\n",
    "\n",
    "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. The Encoder class performs the following steps: \n",
    "1. Pass the input through the Embedding layer.\n",
    "2. Scale the embedding by multiplying it by the square root of the embedding dimension. \n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
    "4. Pass the encoded embedding through a dropout layer\n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding dim)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Now it is time to implement the decoder. The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
    "        # Dropout will be applied during training (~1 line).\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x,x,x,attention_mask=look_ahead_mask,return_attention_scores=True)\n",
    "        \n",
    "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
    "        Q1 = self.layernorm1(x+mult_attn_out1)\n",
    "#         print(Q1.shape)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        # Return attention scores as attn_weights_block2 (~1 line) \n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1,enc_output,enc_output,attention_mask=padding_mask,return_attention_scores=True)\n",
    "        \n",
    "        # # apply layer normalization (layernorm2) to the sum of the attention output and the Q from the first block (~1 line)\n",
    "        mult_attn_out2 = self.layernorm2(Q1+mult_attn_out2)\n",
    "       \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output,training=training)\n",
    "        \n",
    "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
    "        out3 = self.layernorm3(ffn_output+mult_attn_out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding_dim=12 and num_heads=16:\n",
      "\n",
      "q has shape:(1, 15, 12)\n",
      "Output of encoder has shape:(1, 7, 8)\n",
      "\n",
      "Output of decoder layer has shape:(1, 15, 12)\n",
      "Att Weights Block 1 has shape:(1, 16, 15, 15)\n",
      "Att Weights Block 2 has shape:(1, 16, 15, 7)\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "key_dim = 12\n",
    "n_heads = 16\n",
    "\n",
    "decoderLayer_test = DecoderLayer(embedding_dim=key_dim, num_heads=n_heads, fully_connected_dim=32)\n",
    "\n",
    "q = np.ones((1, 15, key_dim))\n",
    "encoder_test_output = tf.convert_to_tensor(np.random.rand(1, 7, 8))\n",
    "look_ahead_mask = create_look_ahead_mask(q.shape[1])\n",
    "\n",
    "out, attn_w_b1, attn_w_b2 = decoderLayer_test(x=q, enc_output=encoder_test_output, training=False, look_ahead_mask=look_ahead_mask, padding_mask=None)\n",
    "\n",
    "print(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\n",
    "print(f\"q has shape:{q.shape}\")\n",
    "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
    "\n",
    "print(f\"Output of decoder layer has shape:{out.shape}\")\n",
    "print(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\n",
    "print(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Decoder\n",
    "\n",
    "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers.\n",
    " \n",
    "Here, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your generated output through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "    \n",
    "        # create word embeddings \n",
    "        x = self.embedding(x)\n",
    "       \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # add positional encodings to word embedding\n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "        \n",
    "        # apply a dropout layer to x\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x,training=training)\n",
    "        \n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "            x, block1, block2 = self.dec_layers[i](x,enc_output,training=training,look_ahead_mask=look_ahead_mask,padding_mask=padding_mask)\n",
    "#             print(x.shape)\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att']=block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2_self_att']=block2\n",
    "            \n",
    "#         print(x.shape)\n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num_layers=5, embedding_dim=13 and num_heads=17:\n",
      "\n",
      "x has shape:(3, 4)\n",
      "Output of encoder has shape:(3, 7, 9)\n",
      "\n",
      "Output of decoder has shape:(3, 4, 13)\n",
      "\n",
      "Attention weights:\n",
      "decoder_layer1_block1_self_att has shape:(3, 17, 4, 4)\n",
      "decoder_layer1_block2_self_att has shape:(3, 17, 4, 7)\n",
      "decoder_layer2_block1_self_att has shape:(3, 17, 4, 4)\n",
      "decoder_layer2_block2_self_att has shape:(3, 17, 4, 7)\n",
      "decoder_layer3_block1_self_att has shape:(3, 17, 4, 4)\n",
      "decoder_layer3_block2_self_att has shape:(3, 17, 4, 7)\n",
      "decoder_layer4_block1_self_att has shape:(3, 17, 4, 4)\n",
      "decoder_layer4_block2_self_att has shape:(3, 17, 4, 7)\n",
      "decoder_layer5_block1_self_att has shape:(3, 17, 4, 4)\n",
      "decoder_layer5_block2_self_att has shape:(3, 17, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers = 5\n",
    "emb_d = 13\n",
    "n_heads = 17\n",
    "fully_connected_dim = 16\n",
    "target_vocab_size = 300\n",
    "maximum_position_encoding = 6\n",
    "\n",
    "x = np.array([[3, 2, 1, 1], [2, 1, 1, 0], [2, 1, 1, 0]])\n",
    "\n",
    "encoder_test_output = tf.convert_to_tensor(np.random.rand(3, 7, 9))\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(x.shape[1])\n",
    "\n",
    "decoder_test = Decoder(n_layers, emb_d, n_heads, fully_connected_dim, target_vocab_size,maximum_position_encoding)\n",
    "                   \n",
    "outd, att_weights = decoder_test(x, encoder_test_output, training=False, look_ahead_mask=look_ahead_mask,padding_mask= None)\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
    "print(f\"x has shape:{x.shape}\")\n",
    "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
    "\n",
    "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n",
    "print(\"Attention weights:\")\n",
    "for name, tensor in att_weights.items():\n",
    "    print(f\"{name} has shape:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "    \n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
    "    - embedding and positional encoding of your input\n",
    "    - multi-head attention on your input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on your generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output (tf.Tensor): The final output of the model\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(input_sentence,training=training,mask=enc_padding_mask)\n",
    "        \n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(output_sentence,enc_output,training=training,look_ahead_mask=look_ahead_mask,padding_mask=dec_padding_mask)\n",
    "        \n",
    "        # pass decoder output through a linear layer and softmax (~1 line)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
      "\n",
      "sentence_a has shape:(1, 7)\n",
      "sentence_b has shape:(1, 7)\n",
      "\n",
      "Output of transformer (summary) has shape:(1, 7, 350)\n",
      "\n",
      "Attention weights:\n",
      "decoder_layer1_block1_self_att has shape:(1, 17, 7, 7)\n",
      "decoder_layer1_block2_self_att has shape:(1, 17, 7, 7)\n",
      "decoder_layer2_block1_self_att has shape:(1, 17, 7, 7)\n",
      "decoder_layer2_block2_self_att has shape:(1, 17, 7, 7)\n",
      "decoder_layer3_block1_self_att has shape:(1, 17, 7, 7)\n",
      "decoder_layer3_block2_self_att has shape:(1, 17, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers = 3\n",
    "emb_d = 13\n",
    "n_heads = 17\n",
    "fully_connected_dim = 8\n",
    "input_vocab_size = 300\n",
    "target_vocab_size = 350\n",
    "max_positional_encoding_input = 12\n",
    "max_positional_encoding_target = 12\n",
    "\n",
    "transformer = Transformer(n_layers, \n",
    "    emb_d, \n",
    "    n_heads, \n",
    "    fully_connected_dim, \n",
    "    input_vocab_size, \n",
    "    target_vocab_size, \n",
    "    max_positional_encoding_input,\n",
    "    max_positional_encoding_target)\n",
    "\n",
    "# 0 is the padding value\n",
    "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_a)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    training=False,\n",
    "    enc_padding_mask=enc_padding_mask,\n",
    "    look_ahead_mask=look_ahead_mask,\n",
    "    dec_padding_mask=dec_padding_mask\n",
    ")\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
    "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
    "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
    "\n",
    "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
    "print(\"Attention weights:\")\n",
    "for name, tensor in att_weights.items():\n",
    "    print(f\"{name} has shape:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inititalize the model\n",
    "\n",
    "Now that you have defined the model, you can initialize and train it. First you can initialize the model with the parameters below. Note that generally these models are much larger and you are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
    "\n",
    "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads, \n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the transformer\n",
    "\n",
    "The original transformer paper uses Adam optimizer with custom learning rate scheduling, which we define in the cell below. This was empirically shown to produce faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(embedding_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you set up the loss. Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "\n",
    "You will use the sparse categorical cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`) and set the parameter `from_logits` to False since the Transformer does not output raw logits since the last layer has a softmax activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "def masked_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Here you will store the losses, so you can later plot them\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define your custom training function. If you are not very advanced with tensorflow, you can understand this function as an alternative to using `model.compile()` and `model.fit()`, but with added extra flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, inp, tar):\n",
    "    \"\"\"\n",
    "    One training step for the transformer\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input data to summarize\n",
    "        tar (tf.Tensor): Target (summary)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "    dec_padding_mask = create_padding_mask(inp) # Notice that both encoder and decoder padding masks are equal\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = model(\n",
    "            inp,\n",
    "            tar_inp, \n",
    "            training=True, \n",
    "            enc_padding_mask =enc_padding_mask, \n",
    "            look_ahead_mask=look_ahead_mask, \n",
    "            dec_padding_mask=dec_padding_mask\n",
    "        )\n",
    "        loss = masked_loss(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "The last thing you will implement is inference. With this, you will be able to produce actual summaries of the documents. You will use a simple method called greedy decoding, which means you will predict one word at a time and append it to the output. You will start with an `[SOS]` token and repeat the word by word inference until the model returns you the `[EOS]` token or until you reach the maximum length of the sentence (you need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(model, encoder_input, output):\n",
    "    \"\"\"\n",
    "    Helper function for summarization that uses the model to predict just the next word.\n",
    "    Arguments:\n",
    "        encoder_input (tf.Tensor): Input data to summarize\n",
    "        output (tf.Tensor): (incomplete) target (summary)\n",
    "    Returns:\n",
    "        predicted_id (tf.Tensor): The id of the predicted word\n",
    "    \"\"\"\n",
    "    # Create a padding mask for the input (encoder)\n",
    "    enc_padding_mask = create_padding_mask(encoder_input)\n",
    "    # Create a look-ahead mask for the output\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
    "    # Create a padding mask for the input (decoder)\n",
    "    dec_padding_mask = create_padding_mask(encoder_input)\n",
    "\n",
    "    # Run the prediction of the next word with the transformer model\n",
    "    predictions, attention_weights = model(\n",
    "        encoder_input,\n",
    "        output,\n",
    "        training=False,\n",
    "        enc_padding_mask=enc_padding_mask,\n",
    "        look_ahead_mask=look_ahead_mask,\n",
    "        dec_padding_mask=dec_padding_mask\n",
    "    )\n",
    "    predictions = predictions[: ,-1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    return predicted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uday/miniconda3/envs/tf_2.18/lib/python3.9/site-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: [[1940]]\n",
      "Predicted word: cancel\n"
     ]
    }
   ],
   "source": [
    "# Take a random sentence as an input\n",
    "input_document = tokenizer.texts_to_sequences([\"a random sentence\"])\n",
    "input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "# Take the start of sentence token as the only token in the output to predict the next word\n",
    "output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
    "\n",
    "# predict the next word with your function\n",
    "predicted_token = next_word(transformer, encoder_input, output)\n",
    "print(f\"Predicted token: {predicted_token}\")\n",
    "\n",
    "predicted_word = tokenizer.sequences_to_texts(predicted_token.numpy())[0]\n",
    "print(f\"Predicted word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, input_document):\n",
    "    \"\"\"\n",
    "    A function for summarization using the transformer model\n",
    "    Arguments:\n",
    "        input_document (tf.Tensor): Input data to summarize\n",
    "    Returns:\n",
    "        _ (str): The summary of the input_document\n",
    "    \"\"\"    \n",
    "    input_document = tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "    \n",
    "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
    "    \n",
    "    for i in range(decoder_maxlen):\n",
    "        predicted_id = next_word(model, encoder_input, output)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
    "            break\n",
    "\n",
    "    return tokenizer.sequences_to_texts(output.numpy())[0]  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
      "\n",
      "Model written summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[SOS] cancel sixtynine sixtynine sixtynine advises graetful uoft sandstorm ellie's aligning aligning aligning aligning barb barb trimmings scaffoldings 128gb emerson prosecuted kayla's gabi acetone acetone bourgeois waitresses uninvited splitting success hurts calluses likke waitresses uninvited likke tuesdays eek miserable likke tuesdays eek tuesdays acetone labialized labialized riki honney likke uuu bred\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "summarize(transformer, document[training_set_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:59:16.459925: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 921/921\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:00:30.417448: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 6.8690\n",
      "Time taken for one epoch: 74.14595341682434 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] tom is going to the new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new\n",
      "\n",
      "Epoch 2, Batch 921/921\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:01:37.897038: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss 5.7932\n",
      "Time taken for one epoch: 63.422964096069336 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] tom and tom are going to the party for the party at the party [EOS]\n",
      "\n",
      "Epoch 3, Loss 5.484721\n",
      "Time taken for one epoch: 63.68301820755005 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] tom is going to the new job [EOS]\n",
      "\n",
      "Epoch 4, Loss 5.294521\n",
      "Time taken for one epoch: 63.34865379333496 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:03:46.677606: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [SOS] tom is going to buy a new year's eve [EOS]\n",
      "\n",
      "Epoch 5, Loss 5.132921\n",
      "Time taken for one epoch: 63.647510290145874 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] mike and mary are going to the party with her [EOS]\n",
      "\n",
      "Epoch 6, Loss 4.976821\n",
      "Time taken for one epoch: 63.55939507484436 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] anna is going to the party with her [EOS]\n",
      "\n",
      "Epoch 7, Loss 4.817421\n",
      "Time taken for one epoch: 62.64049816131592 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] emily is going to meet with her and she is going to meet with her [EOS]\n",
      "\n",
      "Epoch 8, Loss 4.667521\n",
      "Time taken for one epoch: 64.1969347000122 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:08:04.148368: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [SOS] emily is going to meet with her house and she will meet with her [EOS]\n",
      "\n",
      "Epoch 9, Loss 4.524221\n",
      "Time taken for one epoch: 64.04388451576233 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] emma is looking for a dog for her and she will meet with her [EOS]\n",
      "\n",
      "Epoch 10, Loss 4.391921\n",
      "Time taken for one epoch: 64.20036101341248 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is looking for a place to meet with her [EOS]\n",
      "\n",
      "Epoch 11, Loss 4.265321\n",
      "Time taken for one epoch: 64.02613139152527 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] jane is going to meet with her number with her [EOS]\n",
      "\n",
      "Epoch 12, Loss 4.143021\n",
      "Time taken for one epoch: 61.80024313926697 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah has just seen the dog with her number and hannah will meet with her [EOS]\n",
      "\n",
      "Epoch 13, Loss 4.031421\n",
      "Time taken for one epoch: 61.43103837966919 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is going to see a park with her number [EOS]\n",
      "\n",
      "Epoch 14, Loss 3.923321\n",
      "Time taken for one epoch: 62.293065547943115 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has just arrived at the park with her number they will meet at the park [EOS]\n",
      "\n",
      "Epoch 15, Loss 3.818421\n",
      "Time taken for one epoch: 62.09389591217041 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] jane is going to see the park with her number they will meet with amanda and hannah [EOS]\n",
      "\n",
      "Epoch 16, Batch 921/921\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:16:34.456710: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss 3.7222\n",
      "Time taken for one epoch: 62.09886622428894 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] jane is looking for a place for her number to her [EOS]\n",
      "\n",
      "Epoch 17, Loss 3.626521\n",
      "Time taken for one epoch: 62.98169755935669 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] jane is looking for a place to check if she has already seen her number [EOS]\n",
      "\n",
      "Epoch 18, Loss 3.540721\n",
      "Time taken for one epoch: 64.29552125930786 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for a girl to the park for her [EOS]\n",
      "\n",
      "Epoch 19, Loss 3.449821\n",
      "Time taken for one epoch: 64.01574087142944 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is going to have a party with her number they will text her and bring her number [EOS]\n",
      "\n",
      "Epoch 20, Loss 3.371621\n",
      "Time taken for one epoch: 63.92695236206055 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda has a problem with her number but she is not sure about her [EOS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take an example from the test set, to monitor it during training\n",
    "test_example = 0\n",
    "true_summary = summary_test[test_example]\n",
    "true_document = document_test[test_example]\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_loss.reset_state()\n",
    "    number_of_batches=len(list(enumerate(dataset)))\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
    "        train_step(transformer, inp, tar)\n",
    "    \n",
    "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
    "    losses.append(train_loss.result())\n",
    "    \n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    print('Example summarization on the test set:')\n",
    "    print('  True summarization:')\n",
    "    print(f'    {true_summary}')\n",
    "    print('  Predicted summarization:')\n",
    "    print(f'    {summarize(transformer, true_document)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize some Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] neville: hi there, does anyone remember what date i got married on?  don: are you serious?  neville: dead serious. we're on vacation, and tina's mad at me about something. i have a strange suspicion that this might have something to do with our wedding anniversary, but i have nowhere to check.  wyatt: hang on, i'll ask my wife.  don: haha, someone's in a lot of trouble :d  wyatt: september 17. i hope you remember the year ;) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] wyatt reminds neville his wedding anniversary is on the 17th of september. neville's wife is upset and it might be because neville forgot about their anniversary. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] ollie is on the date of the wedding post is on the wedding way is on the wedding way and he will ask don [EOS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_set_example = 5\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document[training_set_example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set example:\n",
      "[SOS] will: hey babe, what do you want for dinner tonight?  emma:  gah, don't even worry about it tonight  will: what do you mean? everything ok?  emma: not really, but it's ok, don't worry about cooking though, i'm not hungry  will: well what time will you be home?  emma: soon, hopefully  will: you sure? maybe you want me to pick you up?  emma: no no it's alright. i'll be home soon, i'll tell you when i get home.   will: alright, love you.   emma: love you too.  [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] emma will be home soon and she will let will know. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] emma will pick up up tonight and she will pick her up [EOS]\n"
     ]
    }
   ],
   "source": [
    "test_set_example = 3\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(document_test[test_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary_test[test_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document_test[test_set_example]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you critically examine the output of the model, you can notice a few things:\n",
    " - In the training set the model output is (almost) identical to the real output (already after 20 epochs and even more so with more epochs). This might be because the training set is relatively small and the model is relatively big and has thus learned the sentences in the training set by heart (overfitting).\n",
    " - While the performance on the training set looks amazing, it is not so good on the test set. The model overfits, but fails to generalize. Again an easy candidate to blame is the small training set and a comparatively large model, but there might be a variety of other factors.\n",
    " - Look at the test set example 3 and its summarization. Would you summarize it the same way as it is written here? Sometimes the data may be ambiguous. And the training of **your model can only be as good as your data**.\n",
    "\n",
    "Here you only use a small dataset, to show that something can be learned in a reasonable amount of time in a relatively small environment. Generally, large transformers are trained on more than one task and on very large quantities of data to achieve superb performance. You will learn more about this in the rest of this course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_2.18)",
   "language": "python",
   "name": "tf_2.18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
